{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial: Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "import nltk\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem     import WordNetLemmatizer\n",
    "from nltk.corpus   import stopwords\n",
    "\n",
    "from collections   import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from pathlib       import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Root directory = C:\\Users\\giann\\data-science-core\n"
     ]
    }
   ],
   "source": [
    "# set root directory\n",
    "path_root = Path(\"C:/Users/giann/data-science-core\")\n",
    "os.chdir(path_root)\n",
    "print(f'- Root directory = {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCENE 1: [wind] [clop clop clop] \n",
      "KING ARTHUR: Whoa there!  [clop clop clop] \n",
      "SOLDIER #1: Halt!  Who goes there?\n",
      "ARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\n",
      "SOLDIER #1: Pull the other one!\n",
      "ARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court\n"
     ]
    }
   ],
   "source": [
    "# import dataset\n",
    "path_dataset = path_root / 'dataset/grail.txt'\n",
    "file      = open(path_dataset, mode = 'r') # 'r' is to read\n",
    "scene_one = file.read()\n",
    "file.close()\n",
    "print(scene_one[0:450])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\n"
     ]
    }
   ],
   "source": [
    "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\"\n",
    "print(my_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o Split my_string on sentence endings\n",
      "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n"
     ]
    }
   ],
   "source": [
    "sentence_endings = r\"[.?!]\"\n",
    "print(\"o Split my_string on sentence endings\")\n",
    "print(re.split(sentence_endings, my_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o Find all capitalized words\n",
      "['Let', 'RegEx', 'Won', 'Can', 'Or']\n"
     ]
    }
   ],
   "source": [
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(\"o Find all capitalized words\")\n",
    "print(re.findall(capitalized_words, my_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o Split my_string on spaces\n",
      "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n"
     ]
    }
   ],
   "source": [
    "spaces = r\"\\s+\"\n",
    "print(\"o Split my_string on spaces\")\n",
    "print(re.split(spaces, my_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o Search for the first occurrence of 'coconuts' in scene_one\n",
      "strart index 580 | end index 588\n",
      "pick work in scene_one = coconuts\n"
     ]
    }
   ],
   "source": [
    "match = re.search(\"coconuts\", scene_one)\n",
    "# Print the start and end  of match\n",
    "print(\"o Search for the first occurrence of 'coconuts' in scene_one\")\n",
    "print(f\"strart index {match.start()} | end index {match.end()}\")\n",
    "print(f\"pick work in scene_one = {scene_one[match.start(): match.end()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o Search for anything which is in square brackets\n",
      "<_sre.SRE_Match object; span=(9, 32), match='[wind] [clop clop clop]'>\n"
     ]
    }
   ],
   "source": [
    "pattern1 = r\"\\[.*\\]\"\n",
    "print(\"o Search for anything which is in square brackets\")\n",
    "print(re.search(pattern1, scene_one))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(0, 8), match='SCENE 1:'>\n"
     ]
    }
   ],
   "source": [
    "# Find the script notation at the beginning of the fourth sentence and print it\n",
    "pattern2 = r\"[\\w\\s]+:\"\n",
    "print(re.match(pattern2, scene_one[0:450]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Tokenization with NLTK\n",
    "Necessary to run `nltk.download('punkt')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o The text was splitter in a list containing 1881 elements.\n"
     ]
    }
   ],
   "source": [
    "# Split scene_one in list of strings\n",
    "sentences = sent_tokenize(scene_one)\n",
    "print(f'o The text was splitter in a list containing {len(sentences)} elements.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o list of unique tokens\n",
      "['jokes', 'Heee', 'Hiyah', 'ceremony', 'apologise', \"'round\", 'shrubbery', 'verses', 'about', 'bi-weekly']\n"
     ]
    }
   ],
   "source": [
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[2])\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens  = set(word_tokenize(scene_one))\n",
    "print(\"o list of unique tokens\")\n",
    "print(list(unique_tokens)[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot histogram word length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the script into lines: lines\n",
    "lines = scene_one.split('\\n')\n",
    "# Replace all script lines for speaker\n",
    "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
    "lines = [re.sub(pattern, '', l) for l in lines]\n",
    "# Tokenize each line: tokenized_lines\n",
    "tokenized_lines = [regexp_tokenize(s, '\\w+') for s in lines]\n",
    "# Make a frequency list of lengths: line_num_words\n",
    "line_num_words = [len(t_line) for t_line in tokenized_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEKCAYAAAAcgp5RAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAFApJREFUeJzt3X+QXeV93/H3p0CwA26BsFBZkiNCldokHQsqYxInLQY34UcbkSY0MJ2YephRMoMbu+M6Fcl0bDelg9vEtJ6mzOBALCeuMcG4aAx1SjAEexKDBZFlYUGQg2JkVLQpBkM9Jkb+9o/7bHMj72rv7t6rZR/er5k795znPOec5+hIn3v03HOfk6pCktSvv7HcDZAkTZZBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6Serc0cvdAICTTz651q1bt9zNkKQV5cEHH/yLqpqar95LIujXrVvH9u3bl7sZkrSiJPnzUerZdSNJnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ17SfwydinWbblj5Lp7r714gi2RpJcmr+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOzRv0SV6R5IEkX0zycJL3tfIPJ3k8yY722tDKk+SDSfYk2ZnkrEkfhCRpbqMMavYCcF5VPZ/kGOBzSf5nW/buqrr1kPoXAuvb643A9e1dkrQM5r2ir4Hn2+wx7VWHWWUT8JG23ueBE5KsWnpTJUmLMVIffZKjkuwADgB3VdX9bdE1rXvmuiTHtrLVwBNDq+9rZZKkZTBS0FfVwaraAKwBzk7yw8DVwGuBNwAnAf+mVc9smzi0IMnmJNuTbJ+enl5U4yVJ81vQXTdV9QxwL3BBVe1v3TMvAL8NnN2q7QPWDq22Bnhylm3dUFUbq2rj1NTUohovSZrfKHfdTCU5oU2/EngL8MhMv3uSAJcAu9oq24C3trtvzgGerar9E2m9JGleo9x1swrYmuQoBh8Mt1TVp5J8JskUg66aHcAvtvp3AhcBe4BvAm8bf7MlSaOaN+iraidw5izl581Rv4Crlt40SdI4+MtYSeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6N8oPprqxbssdI9Xbe+3FE26JJB05XtFLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdmzfok7wiyQNJvpjk4STva+WnJbk/yWNJPp7ke1r5sW1+T1u+brKHIEk6nFGu6F8Azquq1wMbgAuSnAO8H7iuqtYDXweubPWvBL5eVX8HuK7VkyQtk3mDvgaeb7PHtFcB5wG3tvKtwCVtelObpy0/P0nG1mJJ0oKM1Eef5KgkO4ADwF3AV4BnqurFVmUfsLpNrwaeAGjLnwW+b5Ztbk6yPcn26enppR2FJGlOIwV9VR2sqg3AGuBs4HWzVWvvs12913cVVN1QVRurauPU1NSo7ZUkLdCC7rqpqmeAe4FzgBOSzDy4ZA3wZJveB6wFaMv/FvD0OBorSVq4Ue66mUpyQpt+JfAWYDdwD/CzrdoVwO1telubpy3/TFV91xW9JOnIGOVRgquArUmOYvDBcEtVfSrJl4Gbk/x74E+AG1v9G4HfSbKHwZX8ZRNotyRpRPMGfVXtBM6cpfzPGPTXH1r+LeDSsbROkrRk/jJWkjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnRnk4+Nok9yTZneThJO9o5e9N8rUkO9rroqF1rk6yJ8mjSX5ykgcgSTq8UR4O/iLwrqp6KMmrgAeT3NWWXVdVvz5cOckZDB4I/kPAq4E/SPKDVXVwnA2XJI1m3iv6qtpfVQ+16eeA3cDqw6yyCbi5ql6oqseBPczyEHFJ0pGxoD76JOuAM4H7W9Hbk+xMclOSE1vZauCJodX2cfgPBknSBI0c9EmOBz4BvLOqvgFcD5wObAD2A78xU3WW1WuW7W1Osj3J9unp6QU3XJI0mpGCPskxDEL+o1V1G0BVPVVVB6vqO8CH+KvumX3A2qHV1wBPHrrNqrqhqjZW1capqamlHIMk6TBGuesmwI3A7qr6wFD5qqFqPw3satPbgMuSHJvkNGA98MD4mixJWohR7rp5E/DzwJeS7GhlvwJcnmQDg26ZvcAvAFTVw0luAb7M4I6dq7zjRpKWz7xBX1WfY/Z+9zsPs841wDVLaJckaUz8Zawkdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1bpRHCb7srNtyx0j19l578YRbIklLN8rDwdcmuSfJ7iQPJ3lHKz8pyV1JHmvvJ7byJPlgkj1JdiY5a9IHIUma2yhdNy8C76qq1wHnAFclOQPYAtxdVeuBu9s8wIXA+vbaDFw/9lZLkkY2b9BX1f6qeqhNPwfsBlYDm4CtrdpW4JI2vQn4SA18Hjghyaqxt1ySNJIFfRmbZB1wJnA/cGpV7YfBhwFwSqu2GnhiaLV9rUyStAxGDvokxwOfAN5ZVd84XNVZymqW7W1Osj3J9unp6VGbIUlaoJGCPskxDEL+o1V1Wyt+aqZLpr0faOX7gLVDq68Bnjx0m1V1Q1VtrKqNU1NTi22/JGkeo9x1E+BGYHdVfWBo0TbgijZ9BXD7UPlb29035wDPznTxSJKOvFHuo38T8PPAl5LsaGW/AlwL3JLkSuCrwKVt2Z3ARcAe4JvA28baYknSgswb9FX1OWbvdwc4f5b6BVy1xHZJksbEIRAkqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHVulIeD35TkQJJdQ2XvTfK1JDva66KhZVcn2ZPk0SQ/OamGS5JGM8oV/YeBC2Ypv66qNrTXnQBJzgAuA36orfPfkhw1rsZKkhZu3qCvqvuAp0fc3ibg5qp6oaoeB/YAZy+hfZKkJVpKH/3bk+xsXTsntrLVwBNDdfa1MknSMlls0F8PnA5sAPYDv9HKM0vdmm0DSTYn2Z5k+/T09CKbIUmaz6KCvqqeqqqDVfUd4EP8VffMPmDtUNU1wJNzbOOGqtpYVRunpqYW0wxJ0ggWFfRJVg3N/jQwc0fONuCyJMcmOQ1YDzywtCZKkpbi6PkqJPkYcC5wcpJ9wHuAc5NsYNAtsxf4BYCqejjJLcCXgReBq6rq4GSaLkkaxbxBX1WXz1J842HqXwNcs5RGSZLGx1/GSlLnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ2bN+iT3JTkQJJdQ2UnJbkryWPt/cRWniQfTLInyc4kZ02y8ZKk+Y1yRf9h4IJDyrYAd1fVeuDuNg9wIbC+vTYD14+nmZKkxZo36KvqPuDpQ4o3AVvb9FbgkqHyj9TA54ETkqwaV2MlSQu32D76U6tqP0B7P6WVrwaeGKq3r5V9lySbk2xPsn16enqRzZAkzWfcX8ZmlrKarWJV3VBVG6tq49TU1JibIUmasdigf2qmS6a9H2jl+4C1Q/XWAE8uvnmSpKVabNBvA65o01cAtw+Vv7XdfXMO8OxMF48kaXkcPV+FJB8DzgVOTrIPeA9wLXBLkiuBrwKXtup3AhcBe4BvAm+bQJslSQswb9BX1eVzLDp/lroFXLXURkmSxsdfxkpS5wx6SeqcQS9JnTPoJalz834Zq7mt23LHSPX2XnvxhFsiSXPzil6SOmfQS1LnDHpJ6pxBL0md88vYI8AvbSUtJ6/oJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ1b0g+mkuwFngMOAi9W1cYkJwEfB9YBe4F/VlVfX1ozJUmLNY4r+jdX1Yaq2tjmtwB3V9V64O42L0laJpPoutkEbG3TW4FLJrAPSdKIlhr0BfyvJA8m2dzKTq2q/QDt/ZQl7kOStARLHdTsTVX1ZJJTgLuSPDLqiu2DYTPAa17zmiU2Q5I0lyVd0VfVk+39APBJ4GzgqSSrANr7gTnWvaGqNlbVxqmpqaU0Q5J0GIsO+iTHJXnVzDTwE8AuYBtwRat2BXD7UhspSVq8pXTdnAp8MsnMdv57VX06yReAW5JcCXwVuHTpzZQkLdaig76q/gx4/Szl/wc4fymNkiSNj7+MlaTOGfSS1DmDXpI6Z9BLUucMeknq3FJ/GatlsG7LHSPV23vtxRNuiaSVwCt6SeqcQS9JnTPoJalzBr0kdc4vY19CRv2SVZIWwit6SeqcQS9JnTPoJalzBr0kdc6gl6TOeddNxyZxF8+4h1VwOAdp8gx6LYjBLK08E+u6SXJBkkeT7EmyZVL7kSQd3kSCPslRwG8CFwJnAJcnOWMS+5IkHd6kum7OBva0B4iT5GZgE/DlCe1PLzHj/n5g3F1GC2mf3VBzsytvZZhU0K8Gnhia3we8cUL7kv6/SXwBPe5tTuLD6KVuuT4QluvcLWTfR+JDcFJBn1nK6q9VSDYDm9vs80keXeS+Tgb+YpHrrkQvp+Pt8ljz/lmLuzzWOcx5rHP82bxkLKJ9857XJR7z949SaVJBvw9YOzS/BnhyuEJV3QDcsNQdJdleVRuXup2V4uV0vB5rnzzWI29Sd918AVif5LQk3wNcBmyb0L4kSYcxkSv6qnoxyduB3weOAm6qqocnsS9J0uFN7AdTVXUncOektj9kyd0/K8zL6Xg91j55rEdYqmr+WpKkFctBzSSpcys66HseZiHJ2iT3JNmd5OEk72jlJyW5K8lj7f3E5W7ruCQ5KsmfJPlUmz8tyf3tWD/evthf8ZKckOTWJI+08/sjvZ7XJP+q/f3dleRjSV7R03lNclOSA0l2DZXNei4z8MGWVzuTnHWk2rlig/5lMMzCi8C7qup1wDnAVe34tgB3V9V64O4234t3ALuH5t8PXNeO9evAlcvSqvH7L8Cnq+q1wOsZHHN35zXJauCXgI1V9cMMbsy4jL7O64eBCw4pm+tcXgisb6/NwPVHqI0rN+gZGmahqv4SmBlmoQtVtb+qHmrTzzEIg9UMjnFrq7YVuGR5WjheSdYAFwO/1eYDnAfc2qp0caxJ/ibwD4AbAarqL6vqGTo9rwxu+HhlkqOB7wX209F5rar7gKcPKZ7rXG4CPlIDnwdOSLLqSLRzJQf9bMMsrF6mtkxUknXAmcD9wKlVtR8GHwbAKcvXsrH6z8AvA99p898HPFNVL7b5Xs7vDwDTwG+3bqrfSnIcHZ7Xqvoa8OvAVxkE/LPAg/R5XofNdS6XLbNWctDPO8xCD5IcD3wCeGdVfWO52zMJSf4xcKCqHhwunqVqD+f3aOAs4PqqOhP4v3TQTTOb1je9CTgNeDVwHIPui0P1cF5HsWx/p1dy0M87zMJKl+QYBiH/0aq6rRU/NfPfvfZ+YLnaN0ZvAn4qyV4GXXDnMbjCP6H9lx/6Ob/7gH1VdX+bv5VB8Pd4Xt8CPF5V01X1beA24Efp87wOm+tcLltmreSg73qYhdZHfSOwu6o+MLRoG3BFm74CuP1It23cqurqqlpTVesYnMfPVNU/B+4BfrZV6+VY/zfwRJK/24rOZzB8d3fnlUGXzTlJvrf9fZ451u7O6yHmOpfbgLe2u2/OAZ6d6eKZuKpasS/gIuBPga8Av7rc7Rnzsf0Yg//W7QR2tNdFDPqu7wYea+8nLXdbx3zc5wKfatM/ADwA7AF+Dzh2uds3pmPcAGxv5/Z/ACf2el6B9wGPALuA3wGO7em8Ah9j8P3DtxlcsV8517lk0HXzmy2vvsTgbqQj0k5/GStJnVvJXTeSpBEY9JLUOYNekjpn0EtS5wx6SeqcQa+XvSTnzoyYOUr5GPZ3yfAAfEnuTbLszxVVvwx6vey0kU+X0yUMRlyVjgiDXitGkl9O8ktt+rokn2nT5yf53TZ9eZIvtfHP3z+07vNJ/l2S+4Efac8yeCTJ54B/OsK+j2tjj3+hDUa2qZX/iyS3Jfl0G3/8Pw6tc2WSP21X7B9K8l+T/CjwU8B/SrIjyemt+qVJHmj1f3xMf2QSYNBrZbkPmAnBjcDxbTygHwM+m+TVDMY6P4/Br0/fkGRmiNjjgF1V9UYGv0r9EPBP2vb+9gj7/lUGQzO8AXgzg6A+ri3bAPwc8PeAn8vgoTGvBv4tg2cJ/CPgtQBV9UcMfgr/7qraUFVfads4uqrOBt4JvGeBfy7SYRn0WkkeBP5+klcBLwB/zCDwfxz4LPAG4N4aDKL1IvBRBmO/AxxkMEAcDEL38ap6rAY/Df/dEfb9E8CWJDuAe4FXAK9py+6uqmer6lsMxnL5fgbPS/jDqnq6BgN6/d48258ZtO5BYN0I7ZFGdvT8VaSXhqr6dhvh8m3AHzEYK+bNwOkMHszyg4dZ/VtVdXB4cwvcfYCfqapH/1ph8kYGHzozDjL4dzXbkLSHM7ONmfWlsfGKXivNfcC/bu+fBX4R2NGuzO8H/mGSk9sXrpcDfzjLNh4BThvqH798hP3+PvAv2yiMJDlznvoPtLac2Ibk/ZmhZc8Brxphn9JYGPRaaT4LrAL+uKqeAr7VyqjBkK9XMxgG94vAQ1X1XUPgti6WzcAd7cvYPx9hv78GHAPsbA+C/rXDVa7B05X+A4MPnz9g0KXzbFt8M/Du9qXu6XNsQhobR6+UJiTJ8VX1fLui/yRwU1V9crnbpZcfr+ilyXlv+/J2F/A4g7HnpSPOK3pJ6pxX9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalz/w9fVFz6ADENVwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot a histogram of the line lengths\n",
    "plt.hist(line_num_words, bins = 30)\n",
    "plt.xlabel('word length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet tokenization with NLTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = ['This is the best #nlp exercise ive found online! #python',\n",
    "          '#NLP is super fun! <3 #learning',\n",
    "          'Thanks @datacamp :) #nlp #python']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#nlp', '#python'] are the hashtag contained in \n",
      " This is the best #nlp exercise ive found online! #python\n"
     ]
    }
   ],
   "source": [
    "pattern1 = r\"#\\w+\"\n",
    "hashtag = regexp_tokenize(tweets[0], pattern1)\n",
    "print(f\"{hashtag} are the hashtag contained in \\n {tweets[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o Pattern that matches both mentions @ and hashtags #\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['@datacamp', '#nlp', '#python']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern2 = r\"[#@]\\w+\"\n",
    "print(\"o Pattern that matches both mentions @ and hashtags #\")\n",
    "regexp_tokenize(tweets[-1], pattern2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]\n"
     ]
    }
   ],
   "source": [
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-ascii tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_text = 'Wann gehen wir Pizza essen? ðŸ• Und fÃ¤hrst du mit Ãœber? ðŸš•'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', 'ðŸ•', 'Und', 'fÃ¤hrst', 'du', 'mit', 'Ãœber', '?', 'ðŸš•']\n"
     ]
    }
   ],
   "source": [
    "all_words = word_tokenize(german_text)\n",
    "print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wann', 'Pizza', 'Und', 'Ãœber']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and print only capital words\n",
    "capital_words = r\"[A-ZÃœ]\\w+\"\n",
    "print(regexp_tokenize(german_text, capital_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ðŸ•', 'ðŸš•']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and print only emoji\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(german_text, emoji))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical pre-processing for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the article: `tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16457"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(scene_one)\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the tokens into lowercase: `lower_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_tokens = [t.lower() for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retain alphabetic words: `alpha_only`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_only = [t for t in lower_tokens if t.isalpha()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all stop words: `no_stops`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "no_stops = [t for t in alpha_only if t not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the `wordnet_lemmatizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatize all tokens into a new list: `lemmatized`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Bag of Words & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('arthur', 261), ('knight', 149), ('oh', 112), ('launcelot', 101), ('galahad', 80), ('father', 75), ('sir', 71), ('bedevere', 67), ('guard', 65), ('head', 65)]\n"
     ]
    }
   ],
   "source": [
    "bow = Counter(lemmatized)\n",
    "print(bow.most_common(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
